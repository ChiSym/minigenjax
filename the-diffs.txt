diff --git a/src/minigenjax/minigenjax.py b/src/minigenjax/minigenjax.py
index 6602edb..5d4ca54 100644
--- a/src/minigenjax/minigenjax.py
+++ b/src/minigenjax/minigenjax.py
@@ -126,12 +126,13 @@ class Gen[R]:
 
     def vmap(self, in_axes: InAxesT = 0):
         this = self
+
         class ToVmap(Gen):
             def __init__(self, in_axes):
                 self.in_axes = in_axes
 
             def __call__(self, *args):
-                return  VmapA(self.in_axes, args, this)
+                return VmapA(self.in_axes, args, this)
 
         return ToVmap(in_axes)
 
@@ -156,7 +157,7 @@ class Gen[R]:
 
             def __call__(self, *args):
                 return ScanA(args, this)
-            
+
         return ToScan()
 
     def partial(self, *args):
@@ -183,7 +184,8 @@ class GFA[R]:
         self.partial_args = partial_args
 
         shape = jax.eval_shape(f, *args)
-        self.multiple_results = isinstance(shape, (list, tuple))
+        self.multiple_results = True
+        # self.multiple_results = isinstance(shape, (list, tuple))
         self.structure = jax.tree.structure(shape)
 
     def get_impl(self):
@@ -205,7 +207,10 @@ class GFA[R]:
 
     def __matmul__(self, address: str):
         flat_args, in_tree = jax.tree.flatten(self.get_args())
-        return self.get_impl().bind(*flat_args, at=address, in_tree=in_tree)
+        return jax.tree.unflatten(
+            self.get_structure(),
+            self.get_impl().bind(*flat_args, at=address, in_tree=in_tree),
+        )
 
     def abstract(self, *args, **kwargs):
         return self.get_impl().abstract(*args, **kwargs)
@@ -213,6 +218,9 @@ class GFA[R]:
     def get_args(self):
         return self.partial_args + self.args
 
+    def get_structure(self):
+        return self.structure
+
 
 class PartialA(GFA):
     def __init__(self, partial_args, args, next):
@@ -222,10 +230,11 @@ class PartialA(GFA):
 
     def get_impl(self):
         return self.gfa.get_impl()
-    
+
     def get_args(self):
         return self.partial_args + self.args
 
+
 class ScanA(GFA):
     def __init__(self, args, nxt):
         assert len(args) == 2
@@ -239,6 +248,10 @@ class ScanA(GFA):
     def get_args(self):
         return self.args
 
+    def get_structure(self):
+        return self.gfa.get_structure()
+
+
 class RepeatA[R](GFA):
     def __init__(self, n, arg_tuple, next: Gen):
         self.n = n
@@ -258,6 +271,26 @@ class RepeatA[R](GFA):
     def get_args(self):
         return self.arg_tuple
 
+    def get_structure(self):
+        return self.gfa.get_structure()
+
+
+class OldMapA[R](GFA):
+    def __init__(self, g, arg_tuple, next):
+        self.g = g
+        self.arg_tuple = arg_tuple
+        self.inner = next(*arg_tuple)
+        self.name = f"Map[{g.__name__}, {self.inner.name}]"
+        self.structure = jax.tree.structure(self.g(self.inner @ 'a')['retval'])
+
+    def get_impl(self):
+        return MapGF(self.inner, self.g)
+
+    def get_args(self):
+        return self.arg_tuple
+    
+    def get_structure(self):
+        return self.structure
 
 class MapA[R](GFA):
     def __init__(self, g, arg_tuple, next):
@@ -274,12 +307,11 @@ class MapA[R](GFA):
     
     def __matmul__(self, address: str):
         return self.g(self.inner @ address)
-    
-    # def simulate(self, key: PRNGKeyArray):
-    #     tr = self.inner.simulate(key)
-    #     tr['retval'] = self.g(tr['retval'])
-    #     return tr
 
+    def simulate(self, key: PRNGKeyArray):
+        tr = self.inner.simulate(key)
+        tr['retval'] = self.g(tr['retval'])
+        return tr
 
 class VmapA[R](GFA):
     def __init__(self, in_axes: InAxesT, args: tuple, nxt):
@@ -323,6 +355,9 @@ class VmapA[R](GFA):
     def get_args(self):
         return self.arg_tuple
 
+    def get_structure(self):
+        return self.inner.get_structure()
+
     # def simulate(self, key: PRNGKeyArray):
     #     gfa = self.gen(*self.args)
     #     return VMapB(gfa, self.n, self.in_axes).simulate_p(
@@ -365,15 +400,16 @@ class GFB[R](GenPrimitive):
         super().__init__(f"GFB[{f.__name__}]")
         self.f = f
         self.multiple_results = multiple_results
+        self.multiple_results = True
 
     def concrete(self, *args, **kwargs):
         return self.f(*args)
 
     def abstract(self, *args, **kwargs):
-        return jax.tree.map(jax.core.get_aval, jax.eval_shape(self.f, *args))
-        return jax.core.get_aval(
-            jax.eval_shape(self.f, *args)
-        )
+        return jax.tree.flatten(
+            jax.tree.map(jax.core.get_aval, jax.eval_shape(self.f, *args))
+        )[0]
+        return jax.core.get_aval(jax.eval_shape(self.f, *args))
 
     def simulate_p(
         self,
@@ -404,6 +440,7 @@ class RepeatGF[R](GenPrimitive):
         super().__init__(name)
         self.n = n
         self.inner = inner
+        self.multiple_results = True
 
     def simulate_p(
         self,
@@ -430,7 +467,7 @@ class RepeatGF[R](GenPrimitive):
 
     def abstract(self, *args, **kwargs):
         ia = self.inner.abstract(*args, **kwargs)
-        return ia.update(shape=(self.n,) + ia.shape)
+        return jax.tree.map(lambda a: a.update(shape=(self.n,) + a.shape), ia)
 
 
 class GF[R](GFI[R]):
@@ -454,10 +491,13 @@ class GF[R](GFI[R]):
 
         # with jax.check_tracer_leaks():
         self.jaxpr = self.make_jaxpr(g.f, partial_args + args)
-        self.multiple_results = self.structure.num_leaves > 1
+        # self.multiple_results = self.structure.num_leaves > 1
+        self.multiple_results = True
 
     def abstract(self, *args, **_kwargs):
-        return self.abstract_value if self.multiple_results else self.abstract_value[0]
+        return (
+            self.abstract_value
+        )  # if self.multiple_results else self.abstract_value[0]
 
     def concrete(self, *args, **kwargs):
         return self.f(*jax.tree.unflatten(self.in_tree, args))
@@ -490,7 +530,6 @@ class GF[R](GFI[R]):
         return self.structure
 
 
-
 class Transformation[R]:
     def __init__(self, key: PRNGKeyArray, address: Address, constraint: Constraint):
         self.key = key
@@ -552,7 +591,8 @@ class Transformation[R]:
             params = tuple(jax.util.safe_map(read, eqn.invars))
             ans = self.handle_eqn(eqn, params, bind_params)
             # the return value is currently not flat. (is that the right choice?)
-            if eqn.primitive.multiple_results:
+            # TODO: resolve this debate
+            if True or eqn.primitive.multiple_results:
                 jax.util.safe_map(write, eqn.outvars, jax.tree.flatten(ans)[0])
             else:
                 write(eqn.outvars[0], ans)
@@ -561,7 +601,7 @@ class Transformation[R]:
         if structure is not None:
             retval = jax.tree.unflatten(structure, retval)
         else:
-            retval = retval if len(jaxpr.outvars) > 1 else retval[0]
+            pass
         return self.construct_retval(retval)
 
     def address_from_branch(self, b: jx.core.ClosedJaxpr):
@@ -725,8 +765,8 @@ class ScanGF[R](GenPrimitive):
         address: tuple[str, ...],
         constraint: Constraint,
     ) -> dict:
-        #fixed_args = (arg_tuple[0], jax.tree.map(lambda v: v[0], arg_tuple[1]))
-        #gfi = self.g(*fixed_args)
+        # fixed_args = (arg_tuple[0], jax.tree.map(lambda v: v[0], arg_tuple[1]))
+        # gfi = self.g(*fixed_args)
 
         def step(carry_key, s):
             carry, key = carry_key
@@ -748,7 +788,6 @@ class ScanGF[R](GenPrimitive):
         return self.gfi.get_structure()
 
 
-
 class VmapGF[R](GenPrimitive):
     def __init__(self, inner: VmapA):
         self.inner = inner
@@ -765,10 +804,9 @@ class VmapGF[R](GenPrimitive):
     def abstract(self, *args, **kwargs):
         # TODO: probably wrong, these are flat args, and the indices are vs. tree args
         n = args[self.p_index].shape[self.an_axis]
-        ia = self.inner.abstract(*args, **kwargs)
+        ia = self.inner.inner.abstract(*args, **kwargs)
         return ia.update(shape=(n,) + ia.shape)
 
-
     def simulate_p(
         self,
         key: PRNGKeyArray,
@@ -781,9 +819,9 @@ class VmapGF[R](GenPrimitive):
             lambda key, constraint, arg_tuple: self.inner.inner.get_impl().simulate_p(
                 key, arg_tuple, address, constraint
             ),
-            in_axes=(0, 0, self.inner.in_axes)
+            in_axes=(0, 0, self.inner.in_axes),
         )(jax.random.split(key, n), constraint, arg_tuple)
-    
+
     def assess_p(
         self, arg_tuple: tuple, constraint: Constraint, address: tuple[str, ...]
     ) -> Float:
@@ -795,7 +833,6 @@ class VmapGF[R](GenPrimitive):
         )
         return jnp.sum(score), retval
 
-
     # def concrete(self, *args, **kwargs):
     #     return jax.vmap(
     #         lambda *args: self.gfi.concrete(*jax.tree.flatten(args)[0], **kwargs),
@@ -816,7 +853,7 @@ class VmapGF[R](GenPrimitive):
     #     )
     #     return jnp.sum(score), retval
 
-    # def unflatten(self, flat_args):
+    # def unflatten(self, flat_args):   
     #     return jax.tree.unflatten(self.in_tree, flat_args)
 
     # def get_args(self) -> tuple:
@@ -828,10 +865,12 @@ class MapGF[R, S](GenPrimitive):
         super().__init__(f"Map[{inner.name}, {g.__name__}]")
         self.inner = inner
         self.g = g
+        self.multiple_results = True
 
     def abstract(self, *args, **kwargs):
-        ia = self.inner.abstract(*args, **kwargs)
-        return self.g(ia)
+        ia = self.inner.get_impl().abstract(*args, **kwargs)
+        az = jax.tree.unflatten(self.inner.get_structure(), ia)
+        return jax.tree.flatten(self.g(az))[0]
 
     def simulate_p(
         self,
@@ -845,5 +884,4 @@ class MapGF[R, S](GenPrimitive):
         return out
 
 
-
 # %%
diff --git a/src/minigenjax/test_localization.py b/src/minigenjax/test_localization.py
index 16b4eda..2507504 100644
--- a/src/minigenjax/test_localization.py
+++ b/src/minigenjax/test_localization.py
@@ -89,8 +89,10 @@ def test_localization():
     )
 
     key, sub_key = jax.random.split(key)
-    some_pose = whole_map_prior.simulate(key)["retval"]
-
+    some_pose = whole_map_prior.simulate(sub_key)["retval"]
+    tr = jax.jit(whole_map_prior.simulate)(sub_key)
+    assert jnp.allclose(some_pose.hd, tr["retval"].hd)
+    assert jnp.allclose(some_pose.p, tr["retval"].p)
     sensor_model = sensor_model_one.vmap(in_axes=(None, 0, None))
 
     key, sub_key = jax.random.split(key)
@@ -99,28 +101,28 @@ def test_localization():
         tr["retval"],
         jnp.array(
             [
-                0.67065454,
-                0.581487,
-                0.8832029,
-                1.3153447,
-                1.072247,
-                1.0493912,
-                1.3886665,
-                1.5973471,
-                1.4577864,
-                1.5266284,
-                1.4897541,
-                1.6672342,
-                1.1514298,
-                0.7201601,
-                0.89109415,
-                0.9212124,
-                0.88611394,
-                0.786982,
-                0.6631653,
-                0.48975345,
-                0.70259863,
-            ]
+                1.8319645,
+                1.8249748,
+                2.3729553,
+                2.1042943,
+                1.7308967,
+                1.3865273,
+                0.5385731,
+                0.15848322,
+                0.23405387,
+                0.3614612,
+                0.26093096,
+                0.38295925,
+                0.36778474,
+                0.06698091,
+                0.25884473,
+                0.24397819,
+                0.15396227,
+                0.4171125,
+                0.643867,
+                1.6671362,
+                1.8732764,
+            ],
         ),
     )
 
diff --git a/src/minigenjax/test_minigenjax.py b/src/minigenjax/test_minigenjax.py
index bfebab2..25b388c 100644
--- a/src/minigenjax/test_minigenjax.py
+++ b/src/minigenjax/test_minigenjax.py
@@ -475,7 +475,7 @@ class TestCurve:
             jnp.array([22.00005, 10.993716, 3.9880881, 0.99697554, 1.99467, 7.027452]),
         )
         tr = curve_model.vmap(in_axes=(None, None, 0, None))(
-            f, 0.0, jnp.array([0.001, 0.01, 0.9]), 0.3
+           f, 0.0, jnp.array([0.001, 0.01, 0.9]), 0.3
         ).simulate(key0)
         assert jnp.allclose(tr["subtraces"]["outlier"]["retval"], jnp.array([0, 0, 1]))
         assert jnp.allclose(tr["retval"], jnp.array([0.9980389, 0.91635126, 1.0424924]))
@@ -490,18 +490,28 @@ class TestCurve:
         tr = quadratic.repeat(n=3)().simulate(key0)
         assert isinstance(tr["retval"], Poly)
 
-        @Gen
+        @Gen    
         def one_model(x):
             poly = quadratic() @ "p"
             return poly(x)
-        
-        print(jax.make_jaxpr(one_model(0.0).simulate)(key0))
-        
+
+        # print(jax.make_jaxpr(one_model(0.0).simulate)(key0))
+
         tr = one_model(0.0).simulate(key0)
         assert tr["retval"] == 1.1188384
-        assert isinstance(tr["subtraces"]["p"]["retval"], Poly)
 
-        # curve_model(f, x, p_outlier, sigma_inlier)
+        # assert isinstance(tr["subtraces"]["p"]["retval"], Poly)
+        #
+        # One might wish this were true, but what happens is that JAX tracing
+        # simply takes the output of quadratic and evaluates it at x without
+        # constructing the intermediate polynomial object. This could be 
+        # considered a feature or a bug depending on how you look at it. 
+        # The JAXPR above begins with
+        # { lambda ; a:f32[]. let
+        #    b:f32[3] = RepeatGF[3, coefficient][at=p in_tree=PyTreeDef(())] 
+        # and so the MapGF object is not bound during the jit. This is ultimately
+        # because MapA's __matmul__ operation delegates to the inner function.
+
         @Gen
         def model(xs):
             poly = quadratic() @ "p"
@@ -985,3 +995,5 @@ def test_partial():
     assert tr["retval"] == 9.997942
     tr1 = model.partial(10.0, 0.01)().simulate(key0)
     assert tr1["retval"] == tr["retval"]
+
+# %%
